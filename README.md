# Parallel Diffusion Sampling with Low-Dimensional Alignment <br><sub>Official implementation</sub>

**Abstract**: Diffusion models (DMs) have achieved state-of-the-art generative performance but suffer from high sampling latency due to their sequential denoising nature. Existing solver-based acceleration methods often face significant image quality degradation under a low-latency budget, primarily due to accumulated \textbf{truncation errors} arising from the inability to capture high-curvature trajectory segments. In this paper, we propose the \textbf{Ensemble Parallel Direction solver} (dubbed as \ours), a novel ODE solver that mitigates these errors by incorporating multiple parallel gradient evaluations in each step. Motivated by the geometric insight that sampling trajectories are largely confined to a low-dimensional manifold, \ours~leverages the Mean Value Theorem for vector-valued functions to approximate the integral solution more accurately. Importantly, since the additional gradient computations are independent, they can be \textbf{fully parallelized}, preserving low-latency sampling without additional inference cost. We introduce a \textbf{two-stage optimization framework}. Initially, \ours~optimizes a small set of learnable parameters via a distillation-based approach, ensuring minimal training overhead. We further propose a novel, parameter-efficient \textbf{Reinforcement Learning (RL) fine-tuning} scheme that reformulates the solver as a stochastic Dirichlet policy. Unlike traditional methods that fine-tune the massive backbone, our RL approach operates strictly within the \textbf{low-dimensional solver space}, effectively mitigating reward hacking while enhancing performance in complex text-to-image generation tasks. In addition, our method is flexible and can serve as a plugin (\textbf{\oursplugin}) to improve existing ODE samplers. Extensive experiments demonstrate the effectiveness of \ours. On validation benchmarks, at the same latency level of 5 NFE, the distilled \ours~achieves state-of-the-art FID scores of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26 on LSUN Bedroom, surpassing existing learning-based solvers by a significant margin. On text-to-image benchmarks, our RL-tuned \ours~significantly improves human preference scores on both Stable Diffusion v1.5 and SD3-medium. Notably, it outperforms the official 28-step baseline of SD3-Medium with only 20 steps, effectively bridging the gap between inference efficiency and high-fidelity generation.

## Requirements

*This codebase mainly refers to the codebase of [EDM](https://github.com/NVlabs/edm) as the base environment.* 

To configure your environment, run the following code:

```bash
conda env create -f environment.yml -n epd
conda activate epd
pip install omegaconf gdown
conda install lightning -c conda-forge -y
pip install git+https://github.com/openai/CLIP.git
pip install transformers
pip install taming-transformers
pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers
pip install kornia fairscale piq accelerator timm einops
pip install HPSv2
pip install --upgrade diffusers[torch]
```

Then, we provide code to download some model weights.

1. [SD1.5 model weights](https://huggingface.co/dnwalkup/StableDiffusion-v1-Releases/resolve/main/v1-5-pruned-emaonly.ckpt)

2. [HPSv2.1](https://huggingface.co/xswu/HPSv2/blob/main/HPS_v2.1_compressed.pt)

3. [Aesthetic](https://huggingface.co/haor/aesthetics/resolve/main/sac%2Blogos%2Bava1-l14-linearMSE.pth)

4. [MPS](https://drive.usercontent.google.com/download?id=17qrK_aJkVNM75ZEvMEePpLj6L867MLkN&export=download&authuser=0&confirm=t&uuid=56d957a5-dcbb-4bd4-8c99-c53056009c7f&at=ALWLOp6W0qUwp5vV-v5nIwVY8L9U%3A1764524212974)

5. [ImageReward](https://huggingface.co/THUDM/ImageReward/blob/main/ImageReward.pt)

6. [PickScore](https://github.com/yuvalkirstain/pickscore)

```bash
cd src/ms_coco
wget https://huggingface.co/dnwalkup/StableDiffusion-v1-Releases/resolve/main/v1-5-pruned-emaonly.ckpt
cd ..
cd ..
python download_hpsv2_weights.py --version v2.1
cd weights
wget https://huggingface.co/haor/aesthetics/resolve/main/sac%2Blogos%2Bava1-l14-linearMSE.pth
gdown 17qrK_aJkVNM75ZEvMEePpLj6L867MLkN
cd ..
./download.sh  # ignore any output generated by this script
```

*Note*: SD3-Medium model weights will be automatically downloaded when the relevant code is firstly executed.

## Implementation Guide

It is important to make sure to use local packages regarding to HPS and taming-transformers, whenever the relevant code is executed:

```bash
export PYTHONPATH="$PWD/HPSv2:$PWD/src/taming-transformers:$PYTHONPATH"
```

Look up the commands in [launch.sh](./launch.sh) for RL training, sampling and evaluation.

We also provide a detailed guide for each part below.

### RL Training

To train the EPD-Solver, use our recommended training configuration files: [sd3_512.yaml](./training/ppo/cfgs/sd3_512.yaml), [sd3_1024.yaml](./training/ppo/cfgs/sd3_1024.yaml), [sd15.yaml](./training/ppo/cfgs/sd15.yaml). The detailed description of the parameters is provided in the next session.

For convenience, we provide the distilled EPD predictor state after distillation as the required initial model state for RL: [sd3-512-distilled.pkl](./exps/sd3-512/sd3-512-distilled.pkl), [sd3-1024-distilled.pkl](./exps/sd3-1024/sd3-1024-distilled.pkl), [sd15-distilled.pkl](./exps/sd15/sd15-distilled.pkl).

Use the following command:

```bash
torchrun --master_port=12345 --nproc_per_node=1 -m training.ppo.launch \
    --config training/ppo/cfgs/[sd15.yaml, sd3_512.yaml, sd3_1024.yaml]
```

### Inference

We also provide our best model: 



To use an EPD-Solver to generate images, here is an example:

```bash
## SD1.5
MASTER_PORT=12345 python sample.py \
    --predictor_path exps/[xxxxxx]/export/network-snapshot-export-step000005.pkl \
    --prompt-file src/prompts/test.txt \
    --seeds "0-999" \
    --batch 16 \
    --outdir samples/sd15

## SD3-Medium
python sample_sd3.py --predictor exps/[xxxxxx]/export/network-snapshot-export-step007200.pkl \
  --seeds "0" \
  --outdir samples/sd3 \
  --prompt "..."
```

### Evaluation

We provide six metrics to evaluate generated images, including HPSv2.1, PickScore, ImageReward, CLIP, Aesthetic, MPS. You can find the evaluation script in the bottom of [launch.sh](./launch.sh)

## Parameter Description

**Sampling (sample.py / sample_sd3.py)**

| Parameter | Default | What it controls |
|-----------|---------|------------------|
| `predictor_path` / `--predictor` | required | EPD predictor snapshot (.pkl) to replay. |
| `seeds` | `0-63` (sample.py) / `0-3` (sample_sd3.py) | Seeds list or range; drives batch count. |
| `max_batch_size` | `64` (sample.py) / `4` (sample_sd3.py) | Per-process batch size; combine with `subdirs`/`grid` for saving layout. |
| `prompt` / `prompt-file` | None | Text prompt; `prompt-file` can be .txt or .csv (column `text`). |
| `outdir` | Auto-resolved to `samples/...` | Output directory root; creates 1k-chunked subfolders when `subdirs=True`. |

**Solver metadata (read from predictor checkpoints)**

| Parameter | Default source | Notes |
|-----------|----------------|-------|
| `num_steps` | Predictor ckpt | inference steps; total NFE `2*(num_steps-1)`. |
| `guidance_type` / `guidance_rate` | Predictor ckpt | CFG sampling (e.g., 4.5 for SD3 ppo configs, 7.5 for SD1.5). |
| `schedule_type` / `schedule_rho` | Predictor ckpt | `flowmatch` for SD3, `discrete` for SD1.5. |
| `sigma_min` / `sigma_max` | Predictor or backend | Noise range passed to scheduler. |
| `afs`, `max_order`, `predict_x0`, `lower_order_final` | Predictor ckpt | EPD/DPM solver behavior flags. |

**RL Training configs (`training/ppo/cfgs/*.yaml`)**

| Key | sd3_512 | sd3_1024 | sd15 | Purpose |
|-----|---------|----------|------|---------|
| `data.predictor_snapshot` | `exps/sd3-512/...-distilled.pkl` | `exps/sd3-1024/...-distilled.pkl` | `exps/sd15/...-distilled.pkl` | Starting EPD predictor. |
| `model.dataset_name` | `ms_coco` | `ms_coco` | `ms_coco` | Training dataset tag. |
| `model.guidance_type` | `cfg` | `cfg` | `cfg` | Use classifier-free guidance. |
| `model.guidance_rate` | `4.5` | `4.5` | `7.5` | Guidance strength. |
| `model.backend` | `sd3` | `sd3` | `ldm` | Backbone family. |
| `reward.type` | `multi` | `multi` | `multi` | Multi-head reward aggregation. |
| `reward.multi.weights` | `hps:1.0` (others 0) | same | same | Per-head reward weights. |
| `reward.batch_size` | `4` | `4` | `4` | Reward evaluation batch size. |
| `ppo.rollout_batch_size` | `16` | `8` | `8` | Samples per PPO rollout. |
| `ppo.rloo_k` | `4` | `4` | `4` | RLOO baseline samples. |
| `ppo.ppo_epochs` | `1` | `1` | `1` | PPO epochs per update. |
| `ppo.minibatch_size` | `4` | `4` | `4` | Minibatch size. |
| `ppo.learning_rate` | `7e-5` | `7e-5` | `7e-5` | Optimizer LR. |
| `ppo.clip_range` | `0.2` | `0.2` | `0.2` | PPO clip epsilon. |
| `ppo.kl_coef` / `entropy_coef` | `0.0 / 0.0` | `0.0 / 0.0` | `0.0 / 0.0` | Regularization weights. |
| `ppo.dirichlet_concentration` | `10` | `10` | `20` | Dirichlet policy concentration. |

## Citation
If you find this repository useful, please consider citing the following paper:

```

```
